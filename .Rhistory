xtrain <- data[, c("temperature", "rainfall", "N", "P", "K")]
ytrain <- as.factor(data$label)  # Convert crop names to factors
unique(ytrain)
set.seed(123)  # For reproducibility
model <- randomForest(ytrain ~ ., data = data.frame(ytrain, xtrain))
new_data <- data.frame(
temperature = 22,
rainfall = 270,
P = 55,
N = 69,
K = 38
)
# crop_mapping is used for crop names and their respective cropcode to get output
crop_mapping <- data.frame(CropCode = c(1, 2, 3, 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22), label= c("rice","maize" ,"chickpea","kidneybeans" ," pigeonpeas", " mothbeans" ,"mungbean",
"blackgram", "lentil", " pomegranate" , "banana", " mango" ,"grapes" ,"watermelon",
"muskmelon" ," apple", "orange", "papaya", "coconut" ,"cotton", "jute"," coffee"))
predicted_crop <- predict(model, new_data)
predicted_crop_name <- crop_mapping$label[crop_mapping$CropCode == as.numeric(predicted_crop)]
cat("The predicted crop is ",predicted_crop_name)
# Load the required libraries
library(xgboost)
install.packages("xgboost")
install.packages("xgboost")
# Load the required packages
install.packages("randomForest")
library(randomForest)
# Load and preprocess the data (replace 'your_data.csv' with your data file)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$cropname <- as.factor(data$cropname)
install.packages("randomForest")
library(randomForest)
# Load and preprocess the data (replace 'your_data.csv' with your data file)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$cropname <- as.factor(data$cropname)
library(randomForest)
# Load and preprocess the data (replace 'your_data.csv' with your data file)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)  # for reproducibility
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall + humidity + pH, data = train_data, ntree = 500)
library(randomForest)
# Load and preprocess the data (replace 'your_data.csv' with your data file)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)  # for reproducibility
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall + humidity + ph, data = train_data, ntree = 500)
# Make predictions
predictions <- predict(rf_model, test_data)
# Evaluate the model (calculate accuracy)
accuracy <- mean(predictions == test_data$label)
cat("Accuracy:", accuracy, "\n")
# You can also view the confusion matrix to assess performance further
confusion_matrix <- table(Actual = test_data$label, Predicted = predictions)
print(confusion_matrix)
library(randomForest)
# Load and preprocess the data (replace 'your_data.csv' with your data file)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)  # for reproducibility
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall + humidity + ph, data = train_data, ntree = 500)
new_data <- data.frame(N = 50, P = 30, K = 20, temperature = 25, rainfall = 100, humidity = 70, ph = 6)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
# View the predicted crop name
cat("Predicted Crop Name:", predictions, "\n")
# Evaluate the model (calculate accuracy)
accuracy <- mean(predictions == test_data$label)
cat("Accuracy:", accuracy, "\n")
# You can also view the confusion matrix to assess performance further
confusion_matrix <- table(Actual = test_data$label, Predicted = predictions)
library(randomForest)
# Load and preprocess the data (replace 'your_data.csv' with your data file)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)  # for reproducibility
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall + humidity + ph, data = train_data, ntree = 500)
new_data <- data.frame(N = 50, P = 30, K = 20, temperature = 25, rainfall = 100, humidity = 70, ph = 6)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
# Convert numeric predictions to crop names
predicted_crop_name <- levels(data$cropname)[predictions]
# View the predicted crop name
cat("Predicted Crop Name:", predicted_crop_name, "\n")
# Evaluate the model (calculate accuracy)
accuracy <- mean(predictions == test_data$label)
cat("Accuracy:", accuracy, "\n")
# You can also view the confusion matrix to assess performance further
confusion_matrix <- table(Actual = test_data$label, Predicted = predictions)
library(randomForest)
# Load and preprocess the data (replace 'your_data.csv' with your data file)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)  # for reproducibility
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall + humidity + ph, data = train_data, ntree = 500)
new_data <- data.frame(N = 50, P = 30, K = 20, temperature = 25, rainfall = 100, humidity = 70, ph = 6)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
# Convert numeric predictions to crop names
predicted_crop_name <- levels(data$label)[predictions]
# View the predicted crop name
cat("Predicted Crop Name:", predicted_crop_name, "\n")
# Evaluate the model (calculate accuracy)
accuracy <- mean(predictions == test_data$label)
cat("Accuracy:", accuracy, "\n")
# You can also view the confusion matrix to assess performance further
confusion_matrix <- table(Actual = test_data$label, Predicted = predictions)
library(randomForest)
# Load and preprocess the data (replace 'your_data.csv' with your data file)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)  # for reproducibility
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall + humidity + ph, data = train_data, ntree = 500)
new_data <- data.frame(N = 50, P = 30, K = 20, temperature = 25, rainfall = 100)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
library(randomForest)
# Load and preprocess the data (replace 'your_data.csv' with your data file)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)  # for reproducibility
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall, data = train_data, ntree = 500)
new_data <- data.frame(N = 50, P = 30, K = 20, temperature = 25, rainfall = 100)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
# Convert numeric predictions to crop names
predicted_crop_name <- levels(data$label)[predictions]
# View the predicted crop name
cat("Predicted Crop Name:", predicted_crop_name, "\n")
# Evaluate the model (calculate accuracy)
accuracy <- mean(predictions == test_data$label)
cat("Accuracy:", accuracy, "\n")
# You can also view the confusion matrix to assess performance further
confusion_matrix <- table(Actual = test_data$label, Predicted = predictions)
library(randomForest)
# Load and preprocess the data (replace 'your_data.csv' with your data file)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)  # for reproducibility
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall, data = train_data, ntree = 500)
new_data <- data.frame(N = 50, P = 30, K = 20, temperature = 25, rainfall = 100)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
# Convert numeric predictions to crop names
predicted_crop_name <- levels(data$label)[predictions]
# View the predicted crop name
cat("Predicted Crop Name:", predicted_crop_name, "\n")
# Evaluate the model (calculate accuracy)
accuracy <- mean(predictions == test_data$label)
cat("Accuracy:", accuracy, "\n")
install.packages("xgboost")
library(xgboost)
library(caret)
# Load and preprocess the data
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)
train_idx <- createDataPartition(data$label, p = 0.7, list = FALSE)
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Define the training control and train the model
ctrl <- trainControl(method = "cv", number = 5)
xgb_model <- train(label ~ N + P + K + temperature + rainfall, data = train_data, method = "xgbTree", trControl = ctrl)
library(xgboost)
library(caret)
# Load and preprocess the data
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$cropname <- as.factor(data$cropname)
library(xgboost)
library(caret)
# Load and preprocess the data
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)
train_idx <- createDataPartition(data$label, p = 0.7, list = FALSE)
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Define the training control
ctrl <- trainControl(method = "cv", number = 5)
# Train the XGBoost model using the updated API with iteration_range
xgb_model <- train(
label ~ N + P + K + temperature + rainfall,
data = train_data,
method = "xgbTree",
trControl = ctrl,
tuneGrid = expand.grid(
nrounds = c(100, 200, 300),
max_depth = c(3, 4, 5)
)
)
library(xgboost)
library(caret)
# Load and preprocess the data
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "label" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)
train_idx <- createDataPartition(data$label, p = 0.7, list = FALSE)
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Define the training control
ctrl <- trainControl(method = "cv", number = 5)
# Define a simplified parameter grid for hyperparameter tuning
param_grid <- expand.grid(
nrounds = c(100, 200, 300),
max_depth = c(3, 4, 5)
)
# Train the XGBoost model with hyperparameter tuning
xgb_model <- train(
label ~ N + P + K + temperature + rainfall,
data = train_data,
method = "xgbTree",
trControl = ctrl,
tuneGrid = param_grid
)
library(xgboost)
# Load and preprocess the data
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "label" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)
train_idx <- sample(nrow(data), 0.7 * nrow(data))
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create a parameter grid for hyperparameter tuning
param_grid <- expand.grid(
nrounds = c(100, 200, 300),
max_depth = c(3, 4, 5),
eta = c(0.1, 0.01),
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
# Train the XGBoost model with hyperparameter tuning
xgb_model <- xgboost(
data = as.matrix(train_data[, -ncol(train_data)]),  # Exclude the target variable
label = train_data$label,
booster = "gbtree",
eval_metric = "merror",
num_class = length(levels(train_data$label)),
params = list(
objective = "multi:softprob",
eval_metric = "mlogloss"
),
nrounds = 100,  # Initial number of rounds
watchlist = list(train = xgb.DMatrix(train_data), test = xgb.DMatrix(test_data)),
verbose = 0,
print_every_n = 10,  # Adjust as needed
early_stopping_rounds = 10,  # Adjust as needed
maximize = FALSE,
prediction = TRUE,
num_parallel_tree = 1,
param = param_grid
)
library(xgboost)
# Load and preprocess the data
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "label" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)
train_idx <- sample(nrow(data), 0.7 * nrow(data))
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create a parameter grid for hyperparameter tuning
param_grid <- expand.grid(
nrounds = c(100, 200, 300),
max_depth = c(3, 4, 5),
eta = c(0.1, 0.01),
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
# Train the XGBoost model with hyperparameter tuning
xgb_model <- xgboost(
data = as.matrix(train_data[, -ncol(train_data)]),  # Exclude the target variable
label = as.integer(train_data$label) - 1,  # Substract 1 from label to start from 0
nrounds = 100,  # Initial number of rounds
objective = "multi:softmax",
num_class = length(levels(train_data$label)),
eval_metric = "merror",
params = list(
max_depth = 3,  # Set an initial value
eta = 0.1,  # Set an initial value
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
),
verbose = 0,
print_every_n = 10,  # Adjust as needed
early_stopping_rounds = 10,  # Adjust as needed
maximize = FALSE,
num_parallel_tree = 1,
prediction = TRUE,
watchlist = list(train = xgb.DMatrix(train_data, label = as.integer(train_data$label) - 1),
test = xgb.DMatrix(test_data, label = as.integer(test_data$label) - 1))
)
library(randomForest)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "cropname" to a factor
data$cropname <- as.factor(data$cropname)
library(randomForest)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "label" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model using only N, P, K, temperature, and rainfall
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall, data = train_data, ntree = 500)
# Provide input values for prediction
new_data <- data.frame(N = 50, P = 30, K = 20, temperature = 25, rainfall = 100)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
# Convert numeric predictions to crop names
predicted_crop_name <- levels(data$label)[predictions]
# View the predicted crop name
cat("Predicted Crop Name:", predicted_crop_name, "\n")
library(randomForest)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "label" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model using only N, P, K, temperature, and rainfall
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall, data = train_data, ntree = 500)
# Provide input values for prediction
new_data <- data.frame(N = 50, P = 30, K = 20, temperature = 25, rainfall = 100)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
# Convert numeric predictions to crop names
predicted_crop_name <- levels(data$label)[predictions]
# View the predicted crop name
cat("Predicted Crop Name:", predicted_crop_name, "\n")
# Evaluate the model (calculate accuracy)
accuracy <- mean(predictions == test_data$cropname)
cat("Accuracy:", accuracy, "\n")
library(randomForest)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "label" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model using only N, P, K, temperature, and rainfall
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall, data = train_data, ntree = 500)
# Provide input values for prediction
new_data <- data.frame(N = 50, P = 30, K = 20, temperature = 25, rainfall = 100)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
# Convert numeric predictions to crop names
predicted_crop_name <- levels(data$label)[predictions]
# View the predicted crop name
cat("Predicted Crop Name:", predicted_crop_name, "\n")
# Evaluate the model (calculate accuracy)
accuracy <- mean(predictions == test_data$label)
cat("Accuracy:", accuracy, "\n")
library(randomForest)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "label" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model using only N, P, K, temperature, and rainfall
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall+humidity+ph, data = train_data, ntree = 500)
# Provide input values for prediction
new_data <- data.frame(N = 50, P = 30, K = 20, temperature = 25, rainfall = 100,humidity=80,ph=6)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
# Convert numeric predictions to crop names
predicted_crop_name <- levels(data$label)[predictions]
# View the predicted crop name
cat("Predicted Crop Name:", predicted_crop_name, "\n")
# Evaluate the model (calculate accuracy)
accuracy <- mean(predictions == test_data$label)
cat("Accuracy:", accuracy, "\n")
library(randomForest)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "label" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model using only N, P, K, temperature, and rainfall
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall+humidity+ph, data = train_data, ntree = 500)
# Provide input values for prediction
new_data <- data.frame(N = 60, P = 40, K = 80, temperature = 25, rainfall = 100,humidity=80,ph=6)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
# Convert numeric predictions to crop names
predicted_crop_name <- levels(data$label)[predictions]
# View the predicted crop name
cat("Predicted Crop Name:", predicted_crop_name, "\n")
# Evaluate the model (calculate accuracy)
accuracy <- mean(predictions == test_data$label)
cat("Accuracy:", accuracy, "\n")
library(randomForest)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri3/Crops.csv")
# Convert "label" to a factor
data$label <- as.factor(data$label)
# Split the data into a training set and a testing set
set.seed(123)
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model using only N, P, K, temperature, and rainfall
rf_model <- randomForest(label ~ N + P + K + temperature + rainfall, data = train_data, ntree = 500)
# Provide input values for prediction
new_data <- data.frame(N = 60, P = 40, K = 80, temperature = 25, rainfall = 100)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
# Convert numeric predictions to crop names
predicted_crop_name <- levels(data$label)[predictions]
# View the predicted crop name
cat("Predicted Crop Name:", predicted_crop_name, "\n")
# Evaluate the model (calculate accuracy)
accuracy <- mean(predictions == test_data$label)
cat("Accuracy:", accuracy, "\n")
library(randomForest)
data <- read.csv("C:/Users/Dheeraj Saini/Desktop/agri2/Cropdata.csv")
# Convert "Crop" to a factor
data$Crop <- as.factor(data$Crop)
# Split the data into a training set and a testing set
set.seed(123)
train_idx <- sample(1:nrow(data), 0.7 * nrow(data))  # 70% for training
train_data <- data[train_idx, ]
test_data <- data[-train_idx, ]
# Create the Random Forest model using only N, P, K, temperature, and rainfall
rf_model <- randomForest(Crop ~ N + P + K + temperature + rainfall, data = train_data, ntree = 500)
# Provide input values for prediction
new_data <- data.frame(N = 60, P = 40, K = 80, temperature = 25, rainfall = 100)
# Make predictions for the input values
predictions <- predict(rf_model, new_data)
# Convert numeric predictions to crop names
predicted_crop_name <- levels(data$Crop)[predictions]
# View the predicted crop name
cat("Predicted Crop Name:", predicted_crop_name, "\n")
# Evaluate the model (calculate accuracy)
accuracy <- mean(predictions == test_data$Crop)
cat("Accuracy:", accuracy, "\n")
shiny::runApp('D:/Dheeraj Saini/Documents/R programs/shinyplots')
